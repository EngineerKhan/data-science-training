{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE WORKING DIRECTORY AND ADD MY LIBRARIES\n",
    "import os\n",
    "os.chdir('./4.Santander_Customer_Satisfaction')\n",
    "import sys\n",
    "sys.path.insert(0, '../mylib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from show_data import print_full\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from operator import itemgetter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC COMMANDS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA FILES\n",
    "training_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "sample_submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "# \"Constant features can lead to errors in some models and obviously provide no information in the training set that can be learned from.\"\n",
    "remove = []\n",
    "\n",
    "for col in training_df.columns:\n",
    "    if training_df[col].values.std() == 0: # pandas.series std() is not correct, use numpy std() instead (.values.std() instead of std())\n",
    "        remove.append(col)\n",
    "\n",
    "training_df.drop(remove, axis=1, inplace=True)\n",
    "test_df.drop(remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "remove = []\n",
    "c = training_df.columns\n",
    "\n",
    "for i in range(len(c)-1):\n",
    "    v = training_df[c[i]].values\n",
    "    for j in range(i+1,len(c)):\n",
    "        if np.array_equal(v,training_df[c[j]].values):\n",
    "            remove.append(c[j])\n",
    "            \n",
    "training_df.drop(remove, axis=1, inplace=True)\n",
    "test_df.drop(remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPLORATORY ANALYSIS:\n",
    "# check if there is any repeated ID, which would imply to tidy the data set:\n",
    "id_counts_df = training_df['ID'].value_counts().sort_index()  # count the number of occurrences of each ID\n",
    "max(id_counts_df)  # if the max value is 1, then there are no repeated IDs = 1 row for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_df.describe()  # describe training set\n",
    "column_names = list(training_df)  # show all column names. Shorter alternative to my_dataframe.columns.values.tolist()\n",
    "#column_names\n",
    "describe_file = open('describe_file.txt', 'w+')\n",
    "describe_file.write(training_df.describe().to_json()) \n",
    "describe_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore each variable individually:\n",
    "\n",
    "# An important factor would be the demographics. Younger generations tend to be less conformist, so they are very\n",
    "# likely to be unhappy clients if there is something wrong. There is no variable named \"age\", so it would be important \n",
    "# to find it. Describe could give us this information, for example, by looking at the mean. var15 has a mean of 33.21. \n",
    "# It sounds like the average age of clients, so lets explore this variable and other variables with similar mean values.\n",
    "\n",
    "# var15:\n",
    "\n",
    "# By looking at the describe table, var15 min = 5, var15 max = 105.\n",
    "# Not very convincing... yet. Do 5 year old kids already have bank accounts? is it usual to find 105 years old people?\n",
    "# We should check which % of values are between 5-18 and 85-105. \n",
    "# If this % consists on the long tails of the distribution, then this might be the age variable. \n",
    "#sns.set(rc={\"figure.figsize\": (16, 8)})\n",
    "#plt = sns.distplot(training_df['var15'], \n",
    "#             hist_kws={\"linewidth\": 1},  # histogram\n",
    "#             rug_kws={\"color\": \"g\"},  # Plot datapoints in an array as sticks on an axis.\n",
    "#             kde_kws={\"color\": \"b\", \"lw\": 2, \"label\": \"mean\"}  # Fit and plot a univariate or bivariate kernel density estimate.\n",
    "#             )\n",
    "\n",
    "# * I cant figure out how to change the number of data points shown in the x axis: instead of 0, 20, 40... etc I would like it to look like: \n",
    "# 0, 10, 20, 30, 40... etc\n",
    "\n",
    "# Well, it seems like a big amount of the population is near 25 years old. It is the right variable. I think Santander offers good deals for \n",
    "# young professionals and recent graduates when it comes to bank accounts. Given the name of the variables, I think this data set corresponds \n",
    "# to the spanish branch of the bank. However, they might have ideas in common with other branches, such as Santander UK. I was student there, \n",
    "# and I remember they were one the banks which made it easier for students to open a bank account (few paperwork involved). \n",
    "# However, the used to charge 5 pounds per month for maintenance. If this trend has been up for years, it could explain why there are \n",
    "# so many young customers, and less older ones. I actually left the bank a month ago because of these monthly fees, so this could be an \n",
    "# explanation for this distribution. The combination young customer + fees might be quite significative for customers to be unsatisfied. \n",
    "# I will keep this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several boolean variables in the dataset. Lets figure out what they mean. First, I will get all their names and the proportion \n",
    "# of 0s and 1s\n",
    "booleans = [col for col in training_df.columns if min(training_df[col]) == 0 if max(training_df[col]) == 1]\n",
    "#list(training_df[booleans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_var1_0:  0.0114575111813\nind_var1:  0.00376216785057\nind_var5_0:  0.958024204157\nind_var5:  0.663759536964\nind_var6_0:  0.000105235464351\nind_var6:  2.63088660879e-05\nind_var8_0:  0.0328334648777\nind_var8:  0.0285977374375\nind_var12_0:  0.0675217048145\nind_var12:  0.0454617205998\nind_var13_0:  0.0522494080505\nind_var13_corto_0:  0.0429360694554\nind_var13_corto:  0.0414759273875\nind_var13_largo_0:  0.010168376743\nind_var13_largo:  0.00999736911339\nind_var13_medio_0:  2.63088660879e-05\nind_var13:  0.0508550381479\nind_var14_0:  0.023651670613\nind_var14:  0.00530123651671\nind_var17_0:  0.00180215732702\nind_var17:  0.00144698763483\nind_var18_0:  2.63088660879e-05\nind_var19:  0.00419626414102\nind_var20_0:  0.00363062352013\nind_var20:  0.00269665877401\nind_var24_0:  0.0423704288345\nind_var24:  0.0378847671665\nind_var25_cte:  0.0264272559853\nind_var26_0:  0.0246382530913\nind_var26_cte:  0.027558537227\nind_var25_0:  0.02363851618\nind_var30_0:  0.995488029466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_var30:  0.732833464878\nind_var31_0:  0.00427519073928\nind_var31:  0.00367008681926\nind_var32_cte:  0.00121020784004\nind_var32_0:  0.0010786635096\nind_var33_0:  0.000749802683504\nind_var33:  0.000631412786109\nind_var34_0:  2.63088660879e-05\nind_var37_cte:  0.0722967640095\nind_var37_0:  0.065259142331\nind_var39_0:  0.880755064457\nind_var40_0:  0.0114180478821\nind_var40:  0.00372270455143\nind_var41_0:  0.879281767956\nind_var44_0:  0.00188108392528\nind_var44:  0.00169692186267\nind_var7_emit_ult1:  3.94632991318e-05\nind_var7_recib_ult1:  0.00269665877401\nind_var10_ult1:  0.0808734543541\nind_var10cte_ult1:  0.0921599579058\nind_var9_cte_ult1:  0.0968692449355\nind_var9_ult1:  0.0859116022099\nind_var43_emit_ult1:  0.0665877400684\nind_var43_recib_ult1:  0.129308076822\nTARGET:  0.0395685345962\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(booleans)):\n",
    "    print(str(booleans[i]) + ': ',sum(training_df[booleans[i]].values)/len(training_df[booleans[i]].values))\n",
    "\n",
    "# I realized the TARGET variable is very unbalanced (value 1 = 0.0395685345962%). I should apply something in order to fix this problem. \n",
    "# That might improve the final results -> Support Vector Classifier with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMATION\n",
    "\n",
    "# remove the ID field from DataFrames, but save first\n",
    "training_IDs = training_df['ID']\n",
    "test_IDs = test_df['ID']\n",
    "\n",
    "training_df = training_df.drop('ID',axis=1)\n",
    "test_df = test_df.drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform age column to: likely to change = 1, unlikely to change = 0. Lets establish the threshold above 40 years old.\n",
    "# http://discuss.analyticsvidhya.com/t/difference-between-map-apply-and-applymap-in-pandas/2365\n",
    "#training_df['var15'] = training_df['var15'].map(lambda x: 1 if x < 40 else 0)\n",
    "#test_df['var15'] = test_df['var15'].map(lambda x: 1 if x < 40 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore more variables:\n",
    "# if we have delta_imp_amort_var18_1y3, delta_imp_amort_var34_1y3, delta_imp_aport_var13_1y3, delta_imp_aport_var17_1y3, delta_imp_aport_var33_1y3,\n",
    "# lets explore just the first and the second one, as they should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var3: most values around zero. I don't find this variable meaningful nor useful\n",
    "# delta_imp_amort_var18_1y3: \" most values around zero\n",
    "# delta_num_aport_var13_1y3: \" most values around zero\n",
    "# imp_amort_var18_ult1: \" most values around zero\n",
    "# imp_amort_var34_ult1: \" most values around zero\n",
    "# imp_aport_var13_hace3: \" most values around zero\n",
    "# imp_compra_var44_hace3: \" most values around zero\n",
    "\n",
    "# Let's try one of the variables chosen by the feature selector: ind_var5. Cool, the distribution seems to be definately more useful. It is a boolean variable.\n",
    "# How does it correlate to the TARGET variable?\n",
    "\n",
    "# Now, let's explore a variable chosen by the feature selector which is not in the list of boolean variables: num_var4: It has values = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "# How does it correlate to the TARGET variable?\n",
    "\n",
    "# Description:\n",
    "#training_df['num_var4'].describe()\n",
    "\n",
    "# Distribution:\n",
    "#sns.set(rc={\"figure.figsize\": (16, 8)})\n",
    "#plt = sns.distplot(training_df['num_var4'], hist_kws={\"linewidth\": 1}, rug_kws={\"color\": \"g\"}, kde_kws={\"color\": \"b\", \"lw\": 2, \"label\": \"mean\"})\n",
    "\n",
    "# Correlation with TARGET variable: lmplot with or without regression line (fit_reg=True or False). A flat regression line (slope = 0) means no correlation:\n",
    "#sns.lmplot(\"num_var4\", \"TARGET\", data=training_df, fit_reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered_columns = sorted(training_df.columns.tolist())\n",
    "#for col in ordered_columns:\n",
    "#    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames to numpy arrays\n",
    "training_array = training_df.values \n",
    "X = training_array[:, :-1] \n",
    "y = training_array[:, -1] \n",
    "\n",
    "test_array = test_df.values \n",
    "X_test = test_array[:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 22: ind_var5\n1 - 51: ind_var30\n2 - 109: num_var30\n3 - 124: num_var42\n4 - 224: num_meses_var5_ult3\n"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "feature_selector = SelectKBest(f_classif, k=5).fit(X, y) \n",
    "support = feature_selector.get_support(indices=True)\n",
    "\n",
    "i = 0 \n",
    "for index in support:\n",
    "    print(str(i) + \" - \" + str(index) + \": \" + str(training_df.columns[index]))\n",
    "    i += 1\n",
    "\n",
    "#X_transformed = feature_selector.transform(X)\n",
    "#X_test_transformed = feature_selector.transform(X_test)\n",
    "\n",
    "# Check the matching:\n",
    "# print(training_df.iloc[73517,280])\n",
    "# print(X_transformed[73517,19])\n",
    "\n",
    "# Back to dataframe format\n",
    "\n",
    "#y = np.transpose(y) # this does not work, as the array is 1-Dimensional: http://stackoverflow.com/questions/5954603/transposing-a-numpy-array\n",
    "#concatenated = np.concatenate((X_transformed, np.transpose([y])), axis=1)\n",
    "\n",
    "#transformed_columns = training_df.columns[support].values.tolist()\n",
    "#transformed_columns.append('TARGET')\n",
    "\n",
    "#transformed_training_df = pd.DataFrame(concatenated, columns=transformed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\nind_var1\nind_var1_0\nind_var30\nind_var5\nind_var5_0\nind_var6_0\nnum_meses_var5_ult3\nnum_var30\nnum_var42\nsaldo_var30\nvar15\nvar36\n"
     ]
    }
   ],
   "source": [
    "# Remove more columns\n",
    "#final_columns = training_df[['var15']+booleans[:-1]].columns\n",
    "\n",
    "# Get the union of the boolean variables and the chosen variables by the feature selector\n",
    "#final_columns = training_df[['var15']+booleans[:-1]].columns | training_df.columns[support]\n",
    "extra_list = ['var15','saldo_var30','var36']\n",
    "# From the feature analysis of the next code block, I test the SVC with similar variables, without improvements:\n",
    "#extra_list = ['var15','saldo_var30','var36','ind_var8','ind_var12','ind_var13','ind_var24','ind_var30']\n",
    "final_columns = training_df[extra_list + booleans[:5]].columns | training_df.columns[support]\n",
    "final_columns = final_columns.tolist()\n",
    "\n",
    "# From the feature analysis of the next code block, I removed those variables which does not seem very significative, given they got most\n",
    "# values around one of the two possible values (boolean variables)\n",
    "#final_columns = [x for x in final_columns if x not in ['ind_var1', 'ind_var1_0', 'ind_var5_0', 'ind_var6_0']]\n",
    "\n",
    "#final_columns.extend(['TARGET']) != final_columns+['TARGET'] # The former doesnt modify final_columns, the latter does.\n",
    "\n",
    "print(len(final_columns))\n",
    "for e in final_columns:\n",
    "    print(e) \n",
    "final_training_df = training_df[final_columns+['TARGET']]\n",
    "final_test_df = test_df[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    76020.000000\nmean        40.449079\nstd         47.362719\nmin          0.000000\n25%          2.000000\n50%          3.000000\n75%         99.000000\nmax         99.000000\nName: var36, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets study these variables:\n",
    "# ind_var1: \n",
    "#   Distribution: it seems to have many zeros and few ones, so this variable might not be very significative.\n",
    "#   Correlation with TARGET: not much\n",
    "# ind_var_0: same as the previous one\n",
    "# ind_var_30:\n",
    "#   Distribution: this one seems to be more balanced, ~75% = 1, 25% = 0\n",
    "#   Correlation with TARGET: not much\n",
    "# ind_var_5:\n",
    "#   Distribution: this one seems to be more balanced\n",
    "#   Correlation with TARGET: not much\n",
    "# ind_var5_0:\n",
    "#   Distribution: very biased to 1\n",
    "#   Correlation with TARGET: not much\n",
    "# ind_var6_0:\n",
    "#   Distribution: very biased to 0\n",
    "#   Correlation with TARGET: not much\n",
    "# num_meses_var5_ult3:\n",
    "#   Distribution: 0,1,2,3 not biased\n",
    "#   Correlation with TARGET: not much\n",
    "# num_var30:\n",
    "#   Distribution: [ 0,  3,  6,  9, 12, 15, 18, 33, 21] -> 0, 3 and 6 are the peaked values\n",
    "#   Correlation with TARGET: interesting\n",
    "# num_var42:\n",
    "#   Distribution: [ 0,  3,  6,  9, 12, 15, 18] -> 0, 3 and 5 are the peaked values\n",
    "#   Correlation with TARGET: interesting\n",
    "# saldo_var30:\n",
    "#   Distribution: [-4942.260000, 3458077.320000]\n",
    "#   Correlation with TARGET: very interesting\n",
    "# var15:\n",
    "#   Distribution: [5, 105] -> mean at 33.21\n",
    "#   Correlation with TARGET: interesting\n",
    "# var36:\n",
    "#   Distribution: [99,  3,  2,  1,  0] -> mean at 40.44, peaked at 1 and 99\n",
    "#   Correlation with TARGET: not much\n",
    "\n",
    "\n",
    "# remove: 'ind_var1','ind_var_0','ind_var_5_0','ind_var_6_0','','',''\n",
    "\n",
    "current_var = 'var36'\n",
    "\n",
    "# Description:\n",
    "training_df[current_var].unique()\n",
    "training_df[current_var].describe()\n",
    "\n",
    "#sns.set(rc={\"figure.figsize\": (16, 8)})\n",
    "# Distribution:\n",
    "#plt = sns.distplot(training_df[current_var], hist_kws={\"linewidth\": 1}, rug_kws={\"color\": \"g\"}, kde_kws={\"color\": \"b\", \"lw\": 2, \"label\": \"mean\"})\n",
    "\n",
    "# Correlation with TARGET variable: lmplot with or without regression line (fit_reg=True or False). A flat regression line (slope = 0) means no correlation:\n",
    "#sns.lmplot(current_var, 'TARGET', data=training_df, fit_reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot with seaborn\n",
    "#sns_plot = sns.pairplot(transformed_training_df, hue='TARGET', size=2.5)\n",
    "#sns_plot.savefig(\"scatterplot10.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames to numpy arrays\n",
    "X_transformed = final_training_df.ix[:, final_training_df.columns != 'TARGET'].values\n",
    "y = final_training_df['TARGET'].values\n",
    "\n",
    "X_test_transformed = final_test_df.ix[:, final_test_df.columns != 'TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the training set:\n",
    "\n",
    "# Get 100 examples with TARGET = 1\n",
    "positives_df = final_training_df[final_training_df['TARGET'] == 1].head(100)\n",
    "\n",
    "# GET 100 examples with TARGET = 0\n",
    "negatives_df = final_training_df[final_training_df['TARGET'] == 0].head(100)\n",
    "\n",
    "# merge the two subsets\n",
    "balanced_training_df = pd.concat([positives_df,negatives_df])\n",
    "\n",
    "# shuffle the rows\n",
    "balanced_training_df = balanced_training_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Transform to numpy arrays\n",
    "X_transformed = balanced_training_df.ix[:, balanced_training_df.columns != 'TARGET'].values\n",
    "y = balanced_training_df['TARGET'].values\n",
    "\n",
    "X_test_transformed = final_test_df.ix[:, final_test_df.columns != 'TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER & TRAINING: support vector machine, class imbalance handling\n",
    "#clf = svm.SVC(kernel='linear', C=1.0)\n",
    "clf = svm.SVC(kernel='linear', class_weight='balanced')\n",
    "clf.fit(X_transformed, y)\n",
    "\n",
    "# TEST: cross-validation\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_transformed, y, test_size=0.4, random_state=0)\n",
    "roc_auc_score(y_test_cv, clf.predict(X_test_cv).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=GaussianNB(), bootstrap=True,\n         bootstrap_features=False, max_features=0.5, max_samples=0.5,\n         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLASSIFIER & TRAINING: bagging with kneighbors\n",
    "#clf = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5) \n",
    "clf = BaggingClassifier(GaussianNB(), max_samples=0.5, max_features=0.5) \n",
    "clf.fit(X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME', base_estimator=GaussianNB(),\n          learning_rate=1.0, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLASSIFIER & TRAINING: adaboost with gaussian naive bayes\n",
    "clf = AdaBoostClassifier(GaussianNB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "\n",
    "clf.fit(X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER & TRAINING: random forest\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER & TRAINING: naive bayes, gaussian\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_transformed, y)  #svm.SVC(kernel='linear', C=1).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset:\n",
    "#X_first_half, X_second_half = np.split(X_transformed, 2, axis=0)\n",
    "#y_first_half, y_second_half = np.split(y, 2, axis=0)\n",
    "\n",
    "X_part = X_transformed[300:450 , :]\n",
    "y_part = y[300:450]\n",
    "\n",
    "sum(y_part)/len(y_part)\n",
    "# 300 -> 450 is a good subset (auc = 0.73442800039992329), with 5.33% of TARGET = 1 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLASSIFIER & TRAINING: support vector machine, class imbalance handling\n",
    "#clf = svm.SVC(kernel='linear', C=1.0)\n",
    "clf = svm.SVC(kernel='linear', class_weight='balanced')\n",
    "clf.fit(X_part, y_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73736381997693734"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST: cross-validation\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_transformed, y, test_size=0.4, random_state=0)\n",
    "roc_auc_score(y_test_cv, clf.predict(X_test_cv).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "prediction = clf.predict(X_test_transformed).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS: preparation\n",
    "result_df = pd.concat([pd.DataFrame(test_IDs), pd.DataFrame(prediction).astype(int)], axis=1)\n",
    "result_df.columns = ['ID', 'TARGET']\n",
    "#result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS: to csv\n",
    "result_df.to_csv('result_SVC_rbf_balanced_3000.csv', index=False, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df.to_csv('result_AdaBoostGNB.csv', index=False, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df.to_csv('result_BaggingGaussianNaiveBayes.csv', index=False, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df.to_csv('result_BaggingKNearestNeighbors.csv', index=False, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df.to_csv('result_GaussianNaiveBayes.csv', index=False, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df.to_csv('result_RandomForest.csv', index=False, dtype=int)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}